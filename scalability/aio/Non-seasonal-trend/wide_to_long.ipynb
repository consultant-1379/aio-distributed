{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions from lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-46c61e02b491>:1: DtypeWarning: Columns (3,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  metadata_store = pd.read_csv('/home/jovyan/work/elbaanh/AIO-dev/data/anonimized2/metadata-anon.csv')\n"
     ]
    }
   ],
   "source": [
    "metadata_store = pd.read_csv('/home/jovyan/work/elbaanh/AIO-dev/data/anonimized2/metadata-anon.csv')\n",
    "# Mixed datatypes in the dimension_name col: floats and OrderedDict as str (need eval)\n",
    "\n",
    "# Get rid of all irrelevant metadata\n",
    "metadata_store = metadata_store[ metadata_store.model_type == 'non_seasonal_trend' ]\n",
    "\n",
    "metadata_store.dimension_name = metadata_store.dimension_name.map(lambda element: eval(element))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"4weeks-lte_enodeb_id-anon.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_name = \"lte_enodeb_id\" # change based on which file is being read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ DF READ ✔️ ------------------------------\n"
     ]
    }
   ],
   "source": [
    "datas = pd.read_csv( os.path.join(\"/home/jovyan/work/elbaanh/AIO-dev/data/anonimized2\", file) )\n",
    "datas.drop(labels='Unnamed: 0', axis = 1, inplace = True)\n",
    "print(\"-\"*30,\"DF READ ✔️\",\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_store = metadata_store[ metadata_store[\"path\"] == file ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting wide data to long \n",
    "Spark needs the data to be in this format for the computations to be parallelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 4000       # Number of timeseries to be in the dataframe for spark processing, change as needed\n",
    "md_indexes = np.random.choice(metadata_store.index, size = num, replace = False)\n",
    "\n",
    "datas2_list = []\n",
    "\n",
    "for _,row in metadata_store.loc[md_indexes].iterrows():\n",
    "    df = datas[ datas[dim_name] == list(row.dimension_name.values())[0]]\n",
    "    datas2_list.append( df[[row.kpi_name,\"dt\",\"ts\",dim_name]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas2 = pd.concat(datas2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert pd data to long format\n",
    "q = pd.melt(datas2, id_vars = [dim_name, \"ts\",\"dt\"])\n",
    "qq = q[[dim_name,\"dt\",\"variable\",\"value\"]].dropna(subset=\"value\").reset_index(drop=True)\n",
    "qq.groupby([dim_name, \"variable\"]).count().index.shape # there is really `num` timeseries in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq.to_parquet(f\"/home/jovyan/work/elbaanh/AIO-dev/data/anonimized2/spark/enodeb_datas_long_pa_{num}.parquet\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
