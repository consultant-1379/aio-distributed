{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eaxxprx/miniconda3/envs/dprophet/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# From original code, commented out imports are not is use\n",
    "import prophet\n",
    "import sys\n",
    "\n",
    "sys.modules['fbprophet'] = prophet\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.plot import add_changepoints_to_plot\n",
    "from fbprophet.diagnostics import cross_validation\n",
    "from fbprophet.diagnostics import performance_metrics\n",
    "\n",
    "import itertools\n",
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from supersmoother import SuperSmoother, LinearSmoother\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "from configs.bad_direction_kpi_dict import bad_direction_kpi_dict\n",
    "from configs.kpi_constraints_dict import kpi_constraints_dict\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "logging.disable(sys.maxsize) #turn off prophet infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "from fbprophet.diagnostics import cross_validation\n",
    "from fbprophet.diagnostics import performance_metrics\n",
    "from fbprophet.diagnostics import generate_cutoffs, single_cutoff_forecast\n",
    "\n",
    "from supersmoother import SuperSmoother\n",
    "\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('prophet')\n",
    "\n",
    "\n",
    "def is_weekday(timestamp):\n",
    "    date = pd.to_datetime(timestamp)\n",
    "    return (date.dayofweek < 5)\n",
    "\n",
    "\n",
    "def run_prophet_funct(df, params, daily_fourier_order, weekly_fourier_order, is_weekend, country_name):\n",
    "    \n",
    "    if is_weekend:\n",
    "        df['weekday'] = df['ds'].apply(is_weekday)\n",
    "        df['weekend'] = ~df['ds'].apply(is_weekday)\n",
    "        m = Prophet(**params, daily_seasonality = False, weekly_seasonality = weekly_fourier_order, uncertainty_samples = 0)\n",
    "        #hozzaadjuk a holidayeket\n",
    "        #weekday, weekend\n",
    "        m.add_seasonality(name='weekday', period=1, fourier_order=daily_fourier_order, condition_name='weekday')\n",
    "        m.add_seasonality(name='weekend', period=1, fourier_order=daily_fourier_order, condition_name='weekend')\n",
    "    else:\n",
    "        m = Prophet(**params, daily_seasonality =daily_fourier_order, weekly_seasonality = weekly_fourier_order, uncertainty_samples = 0)\n",
    "    m.add_country_holidays(country_name=country_name)\n",
    "    m = m.fit(df)\n",
    "    return m\n",
    "    \n",
    "def make_future(model, end, periods):\n",
    "    \"\"\"\n",
    "    end: given in pd.datetime format: not unix timestamp, we start our dataframe at end+1h\n",
    "    model: the prophet model\n",
    "    period: how many hours to forecast\n",
    "    \"\"\"\n",
    "\n",
    "    dates = pd.date_range(start=end+pd.Timedelta('1H'), end = end+pd.Timedelta(str(periods)+'H'), freq = 'H')\n",
    "    dates = np.concatenate((np.array(model.history_dates), dates))\n",
    "\n",
    "    future = pd.DataFrame({'ds': dates})\n",
    "    return future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-d8f3b0c86db2>:1: DtypeWarning: Columns (3,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  metadata_store = pd.read_csv('/home/jovyan/work/elbaanh/AIO-dev/data/anonimized2/metadata-anon.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ DF READ ✔️ ------------------------------\n"
     ]
    }
   ],
   "source": [
    "metadata_store = pd.read_csv(data_loc + 'metadata-anon.csv')\n",
    "# Mixed datatypes in the dimension_name col: floats and OrderedDict as str (need eval)\n",
    "\n",
    "# Get rid of all irrelevant metadata\n",
    "metadata_store = metadata_store[ metadata_store.model_type == 'non_seasonal_trend' ]\n",
    "\n",
    "# evaluate str to OrderedDict\n",
    "# makes things so much easier\n",
    "metadata_store.dimension_name = metadata_store.dimension_name.map(lambda element: eval(element))\n",
    "\n",
    "#params\n",
    "missing_data_percentage_param = 0.3\n",
    "daily_fourier_order = 0\n",
    "weekly_fourier_order = 0\n",
    "is_weekend = False\n",
    "country_name = 'USA'\n",
    "percent = 0.1\n",
    "scores = ['mae'] #['mdape', 'mape', 'smape', 'mae']\n",
    "predictions_write_to = ''\n",
    "errors_write_to = ''\n",
    "write_to = ''\n",
    "alpha = 1.0\n",
    "\n",
    "\n",
    "#infos\n",
    "end = pd.to_datetime(metadata_store['ts'].values[0], unit='s')\n",
    "ts = metadata_store['ts'].values[0]\n",
    "start = end - pd.Timedelta(4, unit = 'w') # \n",
    "files = metadata_store['path'].unique() # arr of unique files\n",
    "\n",
    "files = files[ files != \"4weeks-lte_eci-1614153600.csv\" ] # remove from test list,\n",
    "                                                          # this is just too large\n",
    "\n",
    "\n",
    "print(\"-\"*30,\"DF READ ✔️\",\"-\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "\n",
    "sSmoothing_schema = StructType([StructField(\"y\", FloatType()), \n",
    "                                StructField(\"dt\", DateType()), \n",
    "                                StructField(\"ytop\", FloatType()) ])\n",
    "\n",
    "\n",
    "\n",
    "@pandas_udf(sSmoothing_schema, PandasUDFType.GROUPED_MAP)\n",
    "def sSmoothing(df):\n",
    "\n",
    "    #df.index = df[\"dt\"]    # lehetne a megoldás, ha df.index kell\n",
    "    print(df.columns)\n",
    "    df[\"y\"] = df[\"value\"]\n",
    "    df[\"range\"] = df[\"dt\"]  # df.index\n",
    "    max_range = df.range.max()\n",
    "\n",
    "\n",
    "    model = SuperSmoother()\n",
    "    model.fit(np.array(df.range), df.y, (np.ones(max_range+1)))\n",
    "    \n",
    "    tfit = np.linspace(0, max_range, max_range+1)\n",
    "    yfit = model.predict(tfit)\n",
    "    df[\"ytop\"] = df[\"y\"].copy()\n",
    "    df.y = df.ytop - yfit\n",
    "    \n",
    "    q3, q1 = np.percentile(df.y, [75 ,25])\n",
    "    IQR = q3 - q1\n",
    "    df[\"y\"] = np.where(((df.y < q1-3*IQR)|(df.y > q3+3*IQR)), np.nan, df.y)\n",
    "    df[\"y\"] = df.y.interpolate(method='akima')\n",
    "    df.y = df.y + yfit\n",
    "    df = df.drop(['range'], axis=1)\n",
    "\n",
    "    # Reorder df to evade schema mismatch\n",
    "    df = df[[\"y\",\"dt\", \"ytop\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_schema = StructType([StructField(\"horizon\", IntegerType()),\n",
    "                                StructField(\"mse\", FloatType()),\n",
    "                                StructField(\"rmse\", FloatType()),\n",
    "                                StructField(\"mae\", FloatType()),\n",
    "                                StructField(\"mape\", FloatType()),\n",
    "                                StructField(\"mdape\", FloatType()),\n",
    "                                StructField(\"coverage\", FloatType())])\n",
    "                                #StructField(\"params\", MapType(StringType(), FloatType()))])\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "#@pandas_udf(training_schema, PandasUDFType.GROUPED_MAP)\n",
    "def training(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    daily_fourier_order = 0\n",
    "    weekly_fourier_order = 0\n",
    "    is_weekend = False\n",
    "    country_name = \"USA\"\n",
    "\n",
    "    #df[\"ds\"] = pd.to_datetime(df[\"ts\"] , unit='s')\n",
    "    df[\"ds\"] = df[\"dt\"]\n",
    "    df[\"y\"] = df[\"value\"]\n",
    "    df = df.sort_values('ds')\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df[[\"ds\",\"y\"]]\n",
    "    \n",
    "    \n",
    "\n",
    "    param_grid = {  'changepoint_prior_scale': [0.01, 0.1, 1.0],\n",
    "                        'changepoint_range': [0.8, 0.9, 0.95]       }\n",
    "    all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "\n",
    "    #tuning_results = pd.DataFrame.from_dict({\"horizon\":[1], \"mse\" : [1.5], \"rmse\" :[1.5], \"mae\": [1.5], \"mape\":[1.5], \n",
    "    #                                           \"mdape\":[1.5], \"coverage\": [1.5]}) #! debug\n",
    "\n",
    "\n",
    "    perf_row_list = []\n",
    "\n",
    "    init_horizon = (df.ds.max() - df.ds.min()).days\n",
    "\n",
    "    for params in all_params:\n",
    "        m = run_prophet_funct(df, params, \n",
    "                                daily_fourier_order, \n",
    "                                weekly_fourier_order, is_weekend,\n",
    "                                country_name)\n",
    "\n",
    "        if init_horizon > 15:\n",
    "            df_cv = cross_validation(m,initial = \"14 days\",\n",
    "                                    horizon=\"1 days\",\n",
    "                                    period =\"1 days\")\n",
    "\n",
    "            df_p = performance_metrics(df_cv, rolling_window=1)\n",
    "\n",
    "            perf_row_list.append(df_p)\n",
    "            \n",
    "    if init_horizon > 15:\n",
    "        tuning_results = pd.concat( perf_row_list ).reset_index(drop = True)   #make df out of list of dfs\n",
    "    #tuning_results[\"params\"] = all_params\n",
    "\n",
    "        tuning_results[\"horizon\"] = tuning_results[\"horizon\"].map(lambda timedelta: int(timedelta.total_seconds()) ) #convert horizon of datatype timedelta to integer\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "    healthy_columns = [\"horizon\", \"mse\", \"rmse\", \"mae\", \"mape\", \"mdape\", \"coverage\"]\n",
    "    \n",
    "    \n",
    "\n",
    "    if len(tuning_results.columns) != 7:\n",
    "        for idx, col in enumerate(healthy_columns):\n",
    "            if col not in tuning_results.columns:\n",
    "                tuning_results.insert(idx, col, [69] * len(tuning_results))\n",
    "        \n",
    "        \n",
    "    \n",
    "    return tuning_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_grid_gen(i):\n",
    "    config_grid = {\n",
    "    \"spark.executor.memory\" : [\"2g\", \"4g\", \"7g\" ], \n",
    "    \"spark.executor.instances\" : [\"8\", \"16\", \"32\", \"64\",\"64\"],    #[:i],\n",
    "    \"spark.executor.cores\" : [\"8\", \"4\", \"2\", \"1\", \"2\"],    #[i-1:],\n",
    "    \"spark.driver.memory\" : [\"16g\"][0] #\"64g\"\n",
    "    #\"spark.task.cpus\" : [ \"1\", \"2\"],\n",
    "    #\"spark.python.worker.memory\": [\"1g\", \"5g\"]\n",
    "    }\n",
    "    return config_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_grid = {\n",
    "    \"spark.executor.memory\" : [\"2g\", \"4g\"], \n",
    "    \"spark.executor.instances\" : [\"16\", \"32\", \"64\"],    #[:i],\n",
    "    \"spark.executor.cores\" : [\"8\", \"2\"],    #[i-1:],\n",
    "    \"spark.driver.memory\" : [\"16g\"] #\"64g\"\n",
    "    #\"spark.task.cpus\" : [ \"1\", \"2\"],\n",
    "    #\"spark.python.worker.memory\": [\"1g\", \"5g\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_params = np.random.choice([dict(zip(config_grid.keys(), v)) for v in itertools.product(*config_grid.values())], size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config_params = [ config_grid_gen(i) for i in range(5) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/home/jovyan/work/elbaanh/spark-3.2.1-bin-hadoop3.2/\")  # put the path to pyspark here\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import time, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_statement_gb3 = \"\"\"\n",
    "  SELECT\n",
    "      lte_enodeb_id,\n",
    "      variable,\n",
    "      SUM(value) as value,\n",
    "      dt\n",
    "    FROM spark_datas\n",
    "    GROUP BY lte_enodeb_id, variable, dt\n",
    "    ORDER BY lte_enodeb_id, variable, dt\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! SparkConf has no effect if spark is launched locally, configuration variables need to be set through $SPARK_HOME/conf/spark-defaults.conf\n",
    "cparam = {  'spark.executor.instances': '4', 'spark.executor.memory' : \"16g\",\n",
    "            'spark.executor.cores': '16', 'spark.driver.memory': '120g',\n",
    "            #'spark.memory.offHeap.enabled': 'true', 'spark.memory.offHeap.size': '40g',\n",
    "            #'spark.driver.maxResultsSize':'0' \n",
    "        }\n",
    "\n",
    "\n",
    "try:\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    sc.stop()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "conf = SparkConf()\n",
    "#for k,v in cparam.items(): #! noeffect\n",
    "    #conf.set(k, v)\n",
    "\n",
    "try:\n",
    "    #sc.stop()\n",
    "    sc = SparkContext(conf=conf)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \"\"\"with open(\"/home/jovyan/work/elbaanh/AIO-dev/AIO-non-seasonal-trend-parallel/spark_times.txt\", \"a\") as f:\n",
    "        s = \"infeasible configuration: \" + str(e) + \"\\t\" + json.dumps(cparam)\n",
    "        f.write(s)\"\"\"\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ! change path here\n",
    "spark_datas = spark.read.parquet(f\"/home/jovyan/work/elbaanh/AIO-dev/data/anonimized2/spark/enodeb_datas_long_pa_100.parquet\")   \n",
    "\n",
    "spark_datas.createOrReplaceTempView('spark_datas')\n",
    "spark_datas = spark_datas.withColumn(\"dt\", spark_datas[\"dt\"].cast(TimestampType())) #spark_datas.createOrReplaceTempView('spark_datas')\n",
    "spark_datas.createOrReplaceTempView('spark_datas')\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "try:\n",
    "    # without repartition there's no parallelism\n",
    "    stored_sdf = ( spark.sql(sql_statement_gb3).repartition(spark.sparkContext.defaultParallelism, [\"variable\", \"lte_enodeb_id\"])).cache()\n",
    "\n",
    "\n",
    "    qqqq = stored_sdf.groupby(\"variable\", \"lte_enodeb_id\").applyInPandas(training, training_schema).collect() #\\\n",
    "    \"\"\".write.option(\"header\", True) \\\n",
    "                                                                        .csv(\"/home/jovyan/work/elbaanh/AIO-dev/data/duck/datacsv\")\n",
    "                                                                        #\"\"\"\n",
    "\n",
    "    # TODO kiírni csv-be, write.csv\n",
    "    # TODO rdd <- pandas : map\n",
    "    \n",
    "\n",
    "    \"\"\"with open(\"/home/jovyan/work/elbaanh/AIO-dev/AIO-non-seasonal-trend-parallel/spark_times.txt\", \"a\") as f:\n",
    "        s = str(time.time() - t1 ) + \"  \" + str(size)\n",
    "        f.write(s + \"\\t\" +  json.dumps( cparam ) + \"\\n\")\"\"\"\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \"\"\"with open(\"/home/jovyan/work/elbaanh/AIO-dev/AIO-non-seasonal-trend-parallel/spark_times.txt\", \"a\") as f:\n",
    "        s = str(time.time() - t1 )\n",
    "        f.write(s + \"\\t\" + json.dumps( cparam ) + \"\\t ------- failed: \"+ str(e) +\"\\n\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dprophet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e72a233740408964b8020143c4e5759c07dd6f95d8defc3861e97068946a47f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
