{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prophet\n",
    "import sys\n",
    "sys.modules['fbprophet'] = prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from configs.kpi_constraints_dict import kpi_constraints_dict\n",
    "from configs.bad_direction_kpi_dict import bad_direction_kpi_dict\n",
    "from configs.functions import make_future, run_prophet_funct\n",
    "from configs.functions import add_cond, mult_cond, is_weekday\n",
    "from configs.functions import preprocess_data, hparam_tuning\n",
    "\n",
    "\n",
    "import logging, sys, os\n",
    "import time\n",
    "\n",
    "from uuid import uuid4\n",
    "\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask client init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eaxxprx/miniconda3/envs/nprophet/lib/python3.6/site-packages/distributed/node.py:155: UserWarning:\n",
      "\n",
      "Port 44594 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 53831 instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:53832</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:53831/status' target='_blank'>http://127.0.0.1:53831/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>68</li>\n",
       "  <li><b>Cores: </b>136</li>\n",
       "  <li><b>Memory: </b>17.18 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:53832' processes=68 threads=136, memory=17.18 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client( dashboard_address = ':44594', n_workers = 68, threads_per_worker = 2 ) #scheduler_address=':37243'\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_store = pd.read_csv('/Users/eaxxprx/Desktop/Work/EEA Code/aio/Data/metadata_anon.csv')\n",
    "metadata_store = metadata_store[ metadata_store.model_type == 'seasonal_prophet' ]\n",
    "#metadata_store.dimension_name = metadata_store.dimension_name.map(lambda element: eval(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_store.dimension_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ DF READ ✔️ ------------------------------\n"
     ]
    }
   ],
   "source": [
    "file = \"4weeks-lte_enodeb_id-anon.csv\"\n",
    "datas = pd.read_csv( os.path.join(\"..\", \"data\", file) )\n",
    "print(\"-\"*30,\"DF READ ✔️\",\"-\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose mdrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-1c78be6eeca7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     metadata_store_kpi['dim_str'] = metadata_store_kpi.apply(lambda x: '_'.join(\n\u001b[0;32m---> 21\u001b[0;31m         [str(elem) for elem in x['dimension_name'].keys()]), axis=1)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdim_names_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata_store_kpi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dim_str'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nprophet/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   7550\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7551\u001b[0m         )\n\u001b[0;32m-> 7552\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7554\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nprophet/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nprophet/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nprophet/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                     \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                         \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-1c78be6eeca7>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     metadata_store_kpi['dim_str'] = metadata_store_kpi.apply(lambda x: '_'.join(\n\u001b[0;32m---> 21\u001b[0;31m         [str(elem) for elem in x['dimension_name'].keys()]), axis=1)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdim_names_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata_store_kpi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dim_str'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "mdrow_list = []\n",
    "\n",
    "# arr of unique kpis present in file\n",
    "#kpis = metadata_store.loc[ metadata_store['path'] == file ]['kpi_name'].unique()\n",
    "kpis = metadata_store['kpi_name'].unique()\n",
    "\n",
    "timeseries_counter = 0\n",
    "delayed_result_list = []\n",
    "for kpi in kpis: #test on 5:6\n",
    "    #kpi = \"tcp_tp_ul_sum\"\n",
    "    # mask out part of metadata df\n",
    "    mask =  ((metadata_store['kpi_name'] == kpi) &       \n",
    "            (metadata_store['model_type'] == 'seasonal_prophet') )#Paramesh: 2nd file check\n",
    "\n",
    "    metadata_store_kpi = metadata_store.loc[ mask ]\n",
    "\n",
    "\n",
    "    # in case of mutiple simensions, we need another for here for the keys of the dimension dict e.g.\n",
    "\n",
    "    metadata_store_kpi['dim_str'] = metadata_store_kpi.apply(lambda x: '_'.join(\n",
    "        [str(elem) for elem in x['dimension_name'].keys()]), axis=1)  #Paramesh: removed keys\n",
    "\n",
    "    dim_names_arr = metadata_store_kpi['dim_str'].unique()\n",
    "\n",
    "\n",
    "    for dimension_name in dim_names_arr:\n",
    "        metadata_store_dim = metadata_store_kpi.loc[ metadata_store_kpi['dim_str'] == dimension_name ]\n",
    "\n",
    "        \n",
    "        for _, row in metadata_store_dim.iterrows():\n",
    "            timeseries_counter += 1\n",
    "            rowd = row.to_dict()\n",
    "            \n",
    "            processing_status = {\"ts_uuid\": str(uuid4()), \"failed\": False, \"num_fails\":0, \"last_failed_ts\":None}\n",
    "\n",
    "            row = {**rowd, **processing_status}\n",
    "            #row = pd.concat([x, row])\n",
    "            mdrow_list.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_store_kpi['dimension_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'ts', 'path', 'dimension_name', 'kpi_name',\n",
       "       'missing_data_ratio_all', 'missing_data_ratio_last_week',\n",
       "       'seasonality_flag', 'statonarity_flag', 'missing_data_imputation_flag',\n",
       "       'table', 'nan_trimming_flag', 'ACF_max_difference', 'is_it_constant',\n",
       "       'model_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_store_kpi.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_percentage_param = 0.3\n",
    "DAILY_FOURIER_ORDER = 3\n",
    "WEEKLY_FOURIER_ORDER = 5\n",
    "COUNTRY_NAME = 'USA'\n",
    "percent = 0.1\n",
    "scores = ['mae'] #['mdape', 'mape', 'smape', 'mae']\n",
    "predictions_write_to = '' \n",
    "errors_write_to = ''\n",
    "write_to = 'prophet_results2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = pd.to_datetime(metadata_store['ts'].values[0], unit='s')\n",
    "ts = metadata_store['ts'].values[0]\n",
    "start = end - pd.Timedelta(4, unit = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid =    {  \n",
    "                'changepoint_prior_scale': [0.01, 0.1, 1.0],\n",
    "                'seasonality_prior_scale': [0.1, 1.0, 10.0, 50],\n",
    "                'seasonality_mode' : ['additive', 'multiplicative'],\n",
    "                }\n",
    "\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "all_params2 = [dict(zip(list(param_grid.keys())+['weekend'], v)) for v in itertools.product(*list(param_grid.values())+[['True', 'False']])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ea2ac5b3-668f-44e7-86df-b88351b15268 {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 0.1, 'seasonality_mode': 'additive', 'weekend': 'True'}\n",
      "04c43ce7-9e6b-4596-baa3-95cbca90bebc {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 0.1, 'seasonality_mode': 'additive', 'weekend': 'False'}\n",
      "87397db6-d60c-4875-a613-c82cc0c835f0 {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 0.1, 'seasonality_mode': 'multiplicative', 'weekend': 'True'}\n",
      "8e6b642b-a369-4273-8b34-3a1f5d8d3ad8 {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 0.1, 'seasonality_mode': 'multiplicative', 'weekend': 'False'}\n",
      "f34b4842-917c-4fa2-a89a-096884e2a5a8 {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 1.0, 'seasonality_mode': 'additive', 'weekend': 'True'}\n"
     ]
    }
   ],
   "source": [
    "p_uuid_df = pd.DataFrame( { str(uuid4()): p  for p in all_params2}).T   # p_uuid: 1 id per parameter combination  for tracking - same for all models with the same hparams, regardless of timeseries\n",
    "\n",
    "p_uuid_df.iloc[0].to_dict()\n",
    "for idx, row in p_uuid_df.head().iterrows():\n",
    "    print(idx, row.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>changepoint_prior_scale</th>\n",
       "      <th>seasonality_prior_scale</th>\n",
       "      <th>seasonality_mode</th>\n",
       "      <th>weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ea2ac5b3-668f-44e7-86df-b88351b15268</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>additive</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04c43ce7-9e6b-4596-baa3-95cbca90bebc</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>additive</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87397db6-d60c-4875-a613-c82cc0c835f0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>multiplicative</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8e6b642b-a369-4273-8b34-3a1f5d8d3ad8</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>multiplicative</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f34b4842-917c-4fa2-a89a-096884e2a5a8</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>additive</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     changepoint_prior_scale  \\\n",
       "ea2ac5b3-668f-44e7-86df-b88351b15268                    0.01   \n",
       "04c43ce7-9e6b-4596-baa3-95cbca90bebc                    0.01   \n",
       "87397db6-d60c-4875-a613-c82cc0c835f0                    0.01   \n",
       "8e6b642b-a369-4273-8b34-3a1f5d8d3ad8                    0.01   \n",
       "f34b4842-917c-4fa2-a89a-096884e2a5a8                    0.01   \n",
       "\n",
       "                                     seasonality_prior_scale seasonality_mode  \\\n",
       "ea2ac5b3-668f-44e7-86df-b88351b15268                     0.1         additive   \n",
       "04c43ce7-9e6b-4596-baa3-95cbca90bebc                     0.1         additive   \n",
       "87397db6-d60c-4875-a613-c82cc0c835f0                     0.1   multiplicative   \n",
       "8e6b642b-a369-4273-8b34-3a1f5d8d3ad8                     0.1   multiplicative   \n",
       "f34b4842-917c-4fa2-a89a-096884e2a5a8                       1         additive   \n",
       "\n",
       "                                     weekend  \n",
       "ea2ac5b3-668f-44e7-86df-b88351b15268    True  \n",
       "04c43ce7-9e6b-4596-baa3-95cbca90bebc   False  \n",
       "87397db6-d60c-4875-a613-c82cc0c835f0    True  \n",
       "8e6b642b-a369-4273-8b34-3a1f5d8d3ad8   False  \n",
       "f34b4842-917c-4fa2-a89a-096884e2a5a8    True  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_uuid_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_training(df, row, p_uuid_df):\n",
    "\n",
    "    df_p_dict = {} # store and return performance metrics of models\n",
    "    \n",
    "    # submit tasks to the client\n",
    "    for p_uuid, params_row in p_uuid_df.iterrows(): # we use all_params2 here, which have the weekend param too\n",
    "        #try:\n",
    "        params = params_row.to_dict()\n",
    "        df_p = hparam_tuning(df, params, row,\n",
    "                                is_weekend = True,\n",
    "                                parallel = \"dask\",\n",
    "                                daily_fourier_order=DAILY_FOURIER_ORDER,\n",
    "                                weekly_fourier_order=WEEKLY_FOURIER_ORDER)\n",
    "        #_params = { **params, **{\"weekend\": True}}\n",
    "        df_p_dict[ p_uuid ] = df_p\n",
    "\n",
    "        \n",
    "    return df_p_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdrow_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(mdrow_list).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdrow_df = pd.DataFrame(mdrow_list)[:10]\n",
    "#mdrow_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df_dict = {}\n",
    "for  _, mdrow in mdrow_df[:500].iterrows():\n",
    "    df = dask.delayed( preprocess_data)( datas, mdrow, start, end)\n",
    "    _df_dict[ mdrow.ts_uuid ] = df\n",
    "df_dict = dask.compute( _df_dict )[0]\n",
    "df_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q = submit_training(df_dict[\"66b8eea5-ffe0-4918-82de-5a2b7aff1336\"], mdrow_df.loc[0], p_uuid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(compute['66b8eea5-ffe0-4918-82de-5a2b7aff1336'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'ts_uuid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8310fdbf98f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtaskgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mts_uuid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmdrow_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mts_uuid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_dict\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mts_uuid\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmdrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdrow_df\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mmdrow_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mts_uuid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mts_uuid\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nprophet/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5140\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5141\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'ts_uuid'"
     ]
    }
   ],
   "source": [
    "taskgroups = {}\n",
    "\n",
    "for ts_uuid in mdrow_df.ts_uuid:\n",
    "    df = df_dict[ ts_uuid ]\n",
    "    mdrow = mdrow_df[ mdrow_df.ts_uuid == ts_uuid ]\n",
    "\n",
    "    tg = dask.delayed(submit_training)(df, mdrow, p_uuid_df)\n",
    "    taskgroups[ ts_uuid ] = tg\n",
    "\n",
    "compute = dask.compute(taskgroups)[0]\n",
    "\n",
    "# 500 -> 3400 s\n",
    "# 20  -> 140 s, 210 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "Unfortunately sometimes tasks are stuck pending. This happens mostly on smaller workloads, perhaps the timeout is shorter in these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ [ fut.status != \"finished\" for p_uuid, fut in compute[ts_uuid].items()] for ts_uuid in compute.keys() ]       # task status\n",
    "[ all([ fut.status == \"finished\" for p_uuid, fut in compute[ts_uuid].items()]) for ts_uuid in compute.keys() ]  # taskgroup status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = client.gather(compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([ [ fut.status for p_uuid, fut in compute[ts_uuid].items()] for ts_uuid in compute.keys() ]).explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zookeeper_table = pd.DataFrame(columns = [\"ts_uuid\", \"p_uuid\", \"ts\", \"kpi\", \"dim\", \"params\", \"failed\", \"num_failed\", \"last_failed_ts\"]) \n",
    "# here ts in ts_uuid stands for timeseries, while ts is timestamp. 🦆 me\n",
    "# failed contains the status of the last retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[ p_uuid for p_uuid, fut in compute[ts_uuid].items() if fut.status != \"finished\"] for ts_uuid in compute.keys() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zookeeper_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tg_finished(ts_uuid):\n",
    "    \"\"\"Checking if all tasks are finish\"\"\"\n",
    "    return all([ fut.status == \"finished\" for p_uuid, fut in compute[ts_uuid].items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "# TODO invoke manual retry for failed tasks\n",
    "for ts_uuid, tg in compute.items():\n",
    "\n",
    "    # if all models (tasks) finished for a timeseries (taskgroup), \n",
    "    # and the result was not acquired from distributed before\n",
    "    if tg_finished(ts_uuid) and (ts_uuid not in res):\n",
    "\n",
    "        try: #!\n",
    "            r = { p_uuid: task.result(timeout = 3) for p_uuid, task in tg.items() }     # gather result from distributed by our task keys\n",
    "            for p_uuid in r:\n",
    "                params = p_uuid_df.loc[p_uuid].to_dict()        # ask for params by uuid key\n",
    "                r[ p_uuid ][\"params\"] = pd.Series({0: params})      # 💩\n",
    "            res[ts_uuid] = pd.concat( r ).reset_index(drop = True)\n",
    "        except Exception as e:\n",
    "            print(\"Valami 🐕 nagy hiba van: \", e)\n",
    "\n",
    "    # there is at least 1 failed model (task)\n",
    "    else:\n",
    "\n",
    "\n",
    "        for p_uuid, task in tg.items():\n",
    "\n",
    "\n",
    "            if task.status == \"finished\":\n",
    "                # the task is no longer failed\n",
    "                if (ts_uuid in zookeeper_table.ts_uuid.values) and (p_uuid in zookeeper_table.p_uuid.values): # task was previously in zookeper_table\n",
    "                    mask = ((ts_uuid == zookeeper_table.ts_uuid) & (p_uuid == zookeeper_table.p_uuid))\n",
    "                    zk_row = zookeeper_table[ mask ]\n",
    "                    zk_row.failed = False\n",
    "                    zookeeper_table[ mask ] = zk_row\n",
    "                    # TODO put back finished future into \"compute\" dict\n",
    "                \n",
    "                # the task is finished, but some other tasks in the taskgroup failed: do nothing with task\n",
    "                else:\n",
    "                    pass\n",
    "                                                    \n",
    "            else:\n",
    "                # task has failed and had failed before\n",
    "                if (ts_uuid in zookeeper_table.ts_uuid.values) and (p_uuid in zookeeper_table.p_uuid.values):\n",
    "                    mask = ((ts_uuid == zookeeper_table.ts_uuid) & (p_uuid == zookeeper_table.p_uuid)) # task was previously in zookeper_table\n",
    "                    zk_row = zookeeper_table[ mask ]\n",
    "                    zk_row.num_failed += 1\n",
    "                    zk_row.last_failed_ts = int(time.time())\n",
    "                    zookeeper_table[ mask ] = zk_row\n",
    "                    # TODO do not increase num failed above a certain threshold (say 10), halt manual retries for the task\n",
    "\n",
    "                # new task, first fail\n",
    "                else:\n",
    "                    mdrow = mdrow_df[ mdrow_df.ts_uuid == ts_uuid ]\n",
    "                    task_to_retry = { \"ts_uuid\": ts_uuid, \"p_uuid\": p_uuid, \"ts\": mdrow.ts.values[0], \n",
    "                                    \"kpi\": mdrow.kpi_name.values[0], \"dim\": mdrow.dimension_name.values[0],\n",
    "                                    \"params\": p_uuid_df.loc[p_uuid].to_dict(), \"failed\": True,\n",
    "                                    \"num_failed\": 1, \"last_failed_ts\": int(time.time())}\n",
    "                    zookeeper_table = zookeeper_table.append(pd.Series(task_to_retry), ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_retry = {}\n",
    "for _, zk_row in zookeeper_table.iterrows():\n",
    "    df = df_dict[ zk_row.ts_uuid ]\n",
    "    params = p_uuid_df.loc[ zk_row.p_uuid ].to_dict()\n",
    "    row = mdrow_df[ mdrow_df.ts_uuid == zk_row.ts_uuid ]\n",
    "\n",
    "    if zk_row.ts_uuid not in to_retry:\n",
    "        to_retry[ zk_row.ts_uuid ] = {}\n",
    "\n",
    "    df_p = dask.delayed(hparam_tuning)(df, params, row,\n",
    "                                is_weekend = True,\n",
    "                                parallel = \"dask\",\n",
    "                                daily_fourier_order=DAILY_FOURIER_ORDER,\n",
    "                                weekly_fourier_order=WEEKLY_FOURIER_ORDER)\n",
    "    to_retry[ zk_row.ts_uuid ][ zk_row.p_uuid ] = df_p\n",
    "compute_retry = dask.compute(to_retry)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r[ p_uuid ][\"params\"] = pd.Series({0: params})\n",
    "r[ p_uuid ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#res['6f371025-8dc3-408c-a0a2-ac88c07e02fd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series([ { k[kk].status for kk in compute[k]} for k in compute.keys() ]).explode().value_counts()\n",
    "#rrrr[\"params\"] = all_params[0]\n",
    "rrrr.params = pd.Series({0:all_params2[0]})\n",
    "rrrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zookeeper_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdrow_df[ mdrow_df.ts_uuid == \"443685ea-4430-4a05-a34d-1af6360c37f1\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Az elvarratlan thread-ek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdrow_df[\"failed\"] = mdrow_df.uuid.map( lambda uuid: is_taskgroup_failed( compute[uuid] ) )\n",
    "mdrow_df[\"num_fails\"] = mdrow_df.num_fails.copy() + mdrow_df.failed\n",
    "mdrow_df[\"last_failed_ts\"] = mdrow_df.uuid.map( lambda uuid: datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\") \n",
    "                                                                if is_taskgroup_failed( compute[uuid] )\n",
    "                                                                else mdrow_df[mdrow_df[\"uuid\"]==uuid].last_failed_ts.values[0] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ [ (uuid,all_params2[idx]) for idx in range(len(tg)) if tg[idx][0].status != \"finished\" ] for uuid, tg in compute.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[ client.retry(task) for task in tg if task.status!= \"finished\" ] for tg in compute ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdrow_df[\"failed\"] = [ is_taskgroup_failed(tg) for tg in compute ]\n",
    "#mdrow_df[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import TimeoutError, CancelledError\n",
    "\n",
    "\n",
    "queue_idx = {*zip(range(20), [str(uuid4()) for _ in range(20)])}\n",
    "results = np.empty(shape = 20, dtype = pd.DataFrame)\n",
    "\n",
    "#compute \n",
    "\n",
    "while queue_idx: #legyen valami kilépés ne while \n",
    "    task_list = []\n",
    "    for idx in queue_idx:\n",
    "        task = dask.delayed(submit_training)(df_list[ idx ], mdrow_list[ idx ], all_params)\n",
    "        task_list.append(task)\n",
    "    \n",
    "    queue = dask.compute(task_list)[0]\n",
    "    \n",
    "    time.sleep(30)\n",
    "    for idx in queue_idx:\n",
    "        \n",
    "        results_list = []\n",
    "        taskgroup = queue[ idx ]\n",
    "\n",
    "        ok = True                   # If any task in taskgroup fails -> continue loop: idx++\n",
    "        for task in taskgroup:\n",
    "            try:\n",
    "                result = task.result(timeout = 3)\n",
    "                results_list.append(result)\n",
    "                \n",
    "            except (TimeoutError, CancelledError) as e:\n",
    "                print(idx,\" \",e, sep = \"\")\n",
    "                ok = False\n",
    "                break\n",
    "                \n",
    "        if not ok:\n",
    "            continue\n",
    "\n",
    "        queue_idx.remove( idx )     #remove idx from queue if success\n",
    "\n",
    "        tuning_results = pd.concat( results_list ).reset_index(drop=True)\n",
    "        results[ idx ] = tuning_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_concat = [ pd.concat(elem).reset_index(drop = True) for elem in compute ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_results = results_concat[0].copy()\n",
    "tuning_results[\"params\"] = deepcopy(all_params2)\n",
    "score = \"mae\"\n",
    "\n",
    "myForecast(df_list[0], tuning_results, mdrow_list[0], \"mae\")\n",
    "#tuning_results.params.map(lambda x: x[\"weekend\"] if \"weekend\" in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myForecast(df, tuning_results,\n",
    "               row, score):\n",
    "\n",
    "    kpi = row[\"kpi_name\"]\n",
    "    dim_dict = row[\"dimension_name\"]\n",
    "\n",
    "    multiplicative_condition = False\n",
    "    additive_condition = False\n",
    "    lower = kpi_constraints_dict[kpi][0]\n",
    "    upper = kpi_constraints_dict[kpi][1]\n",
    "\n",
    "    tuning_results[score+'_rank'] = tuning_results[score].rank()\n",
    "    tuning_results['rank'] = tuning_results[score+'_rank']\n",
    "\n",
    "    params = tuning_results.loc[ tuning_results[\"rank\"].idxmin(), \"params\" ]\n",
    "    weeknd = params['weekend']\n",
    "    params.pop(\"weekend\", None)\n",
    "\n",
    "    if weeknd:\n",
    "        m = run_prophet_funct(df, params, \n",
    "                                DAILY_FOURIER_ORDER, \n",
    "                                WEEKLY_FOURIER_ORDER,\n",
    "                                True,\n",
    "                                COUNTRY_NAME)\n",
    "        future = make_future(m, end, 168)\n",
    "        future['weekday'] = future['ds'].apply(is_weekday)\n",
    "        future['weekend'] = ~future['ds'].apply(is_weekday)\n",
    "        forecast = m.predict(future)\n",
    "    \n",
    "    else:\n",
    "        m = run_prophet_funct(df, params, \n",
    "                                DAILY_FOURIER_ORDER, \n",
    "                                WEEKLY_FOURIER_ORDER, \n",
    "                                False, \n",
    "                                COUNTRY_NAME)\n",
    "        future = make_future(m, end, 168)\n",
    "        forecast = m.predict(future)\n",
    "    \n",
    "    if params['seasonality_mode'] == 'multiplicative':\n",
    "        multiplicative_condition = mult_cond(forecast, df, percent, lower, upper, end)# ha mult_cond - > True, akkor NEM multiplikatív\n",
    "        #ellenorizzuk a feltetelek teljesuleset\n",
    "        if multiplicative_condition:\n",
    "            \n",
    "            tuning_results_additive = tuning_results.loc[tuning_results['seasonality_mode']=='additive']\n",
    "            #majd kivalasztjuk a legjobbat\n",
    "            params = tuning_results_additive[list(param_grid.keys())+['weekend']].loc[tuning_results_additive['rank'].idxmin()]\n",
    "            #es tanitunk, vot ma\n",
    "            weeknd = params['weekend']\n",
    "            if weeknd:\n",
    "                m = run_prophet_funct(df, params[:-1], \n",
    "                                        DAILY_FOURIER_ORDER, \n",
    "                                        WEEKLY_FOURIER_ORDER,\n",
    "                                        is_weekend = True, \n",
    "                                        country_name = COUNTRY_NAME)\n",
    "                future = make_future(m, end, 168)\n",
    "                future['weekday'] = future['ds'].apply(is_weekday)\n",
    "                future['weekend'] = ~future['ds'].apply(is_weekday)\n",
    "                forecast = m.predict(future)\n",
    "\n",
    "            else:\n",
    "                m = run_prophet_funct(df, params[:-1], \n",
    "                                        DAILY_FOURIER_ORDER, \n",
    "                                        WEEKLY_FOURIER_ORDER, \n",
    "                                        is_weekend = False, \n",
    "                                        country_name = COUNTRY_NAME)\n",
    "                future = make_future(m, end, 168)\n",
    "                forecast = m.predict(future)\n",
    "    \n",
    "    if params['seasonality_mode'] == 'additive':\n",
    "        additive_condition = add_cond(forecast, percent, lower, upper)\n",
    "        #check conditions and fix\n",
    "        if additive_condition:\n",
    "            if len(m.changepoints[np.abs(np.nanmean(m.params['delta'], axis=0)) >= 0.01].values)==0:\n",
    "                last_changepoint = start\n",
    "            else:\n",
    "                last_changepoint = m.changepoints[np.abs(np.nanmean(m.params['delta'], axis=0)) >= 0.01].values[-1]\n",
    "            minimum = forecast.set_index('ds')[end:]['additive_terms'].quantile(0.05)\n",
    "            maximum = forecast.set_index('ds')[end:]['additive_terms'].quantile(0.95)\n",
    "            last_point =((forecast.set_index('ds')[last_changepoint:]['trend']+minimum>lower)\n",
    "                    & (forecast.set_index('ds')[last_changepoint:]['trend']+maximum<upper))[::-1].idxmax()\n",
    "\n",
    "            forecast.loc[forecast['ds']>last_point, 'trend'] = forecast.loc[forecast['ds']==last_point, 'trend'].values[0]\n",
    "            forecast['yhat'] = (forecast['trend']*(1+forecast['multiplicative_terms'])+forecast['additive_terms'])\n",
    "    \n",
    "    forecast['yhat'] = forecast['yhat'].clip(lower = lower, upper = upper)\n",
    "    \n",
    "    \n",
    "    scaler =  StandardScaler(with_mean = False) # RobustScaler\n",
    "    scaler.fit(df['y'].values.reshape(-1,1))\n",
    "\n",
    "    df = df.set_index(\"ds\")\n",
    "    forecast = forecast.set_index(\"ds\")\n",
    "\n",
    "    results = pd.DataFrame( index = forecast.index,\n",
    "                            columns = [\"kpi_name\", \"dimension_name\", \"ground_truth\",\n",
    "                                        \"pred\", \"error\", \"trend\", \"gt_wo_trend\", \"pred_wo_trend\"])\n",
    "\n",
    "    results[\"kpi_name\"] = [kpi] * len(results)\n",
    "    results[\"dimension_name\"] = [dim_dict] * len(results)\n",
    "    results[\"y_raw\"] = df.y\n",
    "\n",
    "    #kpi név, dim , ts, trend, szezon, zaj, holiday, eredeti y\n",
    "    return forecast\n",
    "\n",
    "    #pd.Series(scaler.transform((df.set_index('ds')['y']-forecast.set_index('ds')['yhat']).values.reshape(-1,1)).T[0], index=forecast['ds'], name = dims_values_str))\n",
    "    #scaler.transform((df['y']-forecast['yhat']).values.reshape(-1,1)).T[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed.client import Future\n",
    "#isinstance(results[0][0], Future)\n",
    "[ client.gather(elem) for elem in compute ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "queue_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"while failed:\n",
    "    for idx in failed:\n",
    "        task_list = dask.delayed(submit_training)(df_list[ idx ], mdrow_list[ idx ], all_params)\n",
    "    retry_queue = dask.compute(task_list)[0]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def gather_or_fail(queue, retried = False):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        queue - list of np.ndarray of dask.distributed.client.Future of unspecified status.\n",
    "        Each element of the np.ndarry is a one-rowed performance_metrics pd.DataFrame.\n",
    "    \n",
    "    Output:\n",
    "        results - np.ndarray of pd.DataFrames.\n",
    "        failed_tasks - list of np.ndarray of failed dask futures with TimeoutError and CancelledError.\n",
    "    \"\"\"\n",
    "    results = np.empty(shape = len(queue), dtype = pd.DataFrame)\n",
    "    failed_tasks = []\n",
    "\n",
    "    cycle_vars = enumerate(queue)\n",
    "\n",
    "    if retried:\n",
    "        cycle_vars = \n",
    "\n",
    "    for idx, task in cycle_vars:\n",
    "        \n",
    "        results_list = []\n",
    "        for subtask in task:\n",
    "            \n",
    "            try:\n",
    "                result = subtask.result(timeout = 3)\n",
    "                \n",
    "            except (TimeoutError, CancelledError) as e:\n",
    "                failed_tasks.append(idx)\n",
    "                print(idx,\" \",e, sep = \"\")\n",
    "                break\n",
    "                \n",
    "            results_list.append(result)\n",
    "        \n",
    "        tuning_results = pd.concat( results_list ).reset_index(drop=True)\n",
    "        results[ idx ] = tuning_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enumerate(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while failed_tasks:\n",
    "    for idx in failed_tasks:\n",
    "        qq = dask.delayed(submit_training)(df_list[ idx ], mdrow_list[ idx ], all_params)\n",
    "    results, failed_tasks = gather_or_fail(qq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [ [  x.result() for x in subarr ] for subarr in q ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq = dask.delayed(submit_training)(df_list[ 16 ], mdrow_list[ 16 ], all_params).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,subarr in enumerate(q):\n",
    "    print([ x.result() for x in subarr])\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [ pd.concat(x).reset_index(drop=True) for x in results ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3s = []\n",
    "for tuning_results in results:\n",
    "    tuning_results[\"params\"] = all_params2\n",
    "    tuning_results[\"rank\"] = tuning_results.mae.rank()\n",
    "    top3s.append( tuning_results.sort_values(\"rank\").head(1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(top3s).reset_index(drop=True).params.map( lambda d: d[\"seasonality_prior_scale\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "q = hparam_tuning(df_list[0], all_params[0], mdrow_list[0],\n",
    "                                    parallel = \"dask\",\n",
    "                                    daily_fourier_order=DAILY_FOURIER_ORDER,\n",
    "                                    weekly_fourier_order=weekly_fourier_order)\n",
    "\n",
    "q.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo\n",
    "\n",
    "TS.model <-> paraméter összekapcsolás"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infos\n",
    "end = pd.to_datetime(metadata_store['ts'].values[0], unit='s')\n",
    "ts = metadata_store['ts'].values[0]\n",
    "start = end - pd.Timedelta(4, unit = 'w')\n",
    "files = set(metadata_store['path'])\n",
    "for file in files:\n",
    "    datas = pd.read_csv(read_from+file)\n",
    "    kpis = set(metadata_store.loc[metadata_store['path']==file]['kpi_name'])\n",
    "    for kpi in kpis:\n",
    "        all_errors =pd.DataFrame(index = pd.date_range(start, end, freq = 'H'))\n",
    "        all_predictions = pd.DataFrame(index = pd.date_range(start, end+pd.Timedelta('1w'), freq = 'H'))\n",
    "        standard_deviations = {}\n",
    "        metadata_store_kpi = metadata_store.loc[(metadata_store['kpi_name']==kpi) & (metadata_store['path']==file) &(metadata_store['model_type']=='seasonal_prophet')]\n",
    "        # in case of mutiple simensions, we need another for here for the keys of the dimension dict e.g.\n",
    "        #metadata_store_kpi['dim_str'] = metadata_store_kpi.apply(lambda x: '_'.join([str(elem) for elem in eval(x['dimension_name']).keys()]))\n",
    "        #dim_names =  set(metadata_store_kpi['dim_str'])\n",
    "        #for dimension_name in dim_names: \n",
    "        for index, row in metadata_store_kpi.loc[metadata_store['model_type']=='seasonal_prophet'].iterrows():\n",
    "\n",
    "                dim_dict = eval(row['dimension_name'])\n",
    "                dims_str = '_'.join([str(elem) for elem in dim_dict.keys()])\n",
    "                dims_values_str = '_'.join([str(elem) for elem in dim_dict.values()])\n",
    "\n",
    "                multiplicative_condition = False\n",
    "                additive_condition = False\n",
    "                lower = kpi_constraints_dict[kpi][0]\n",
    "                upper = kpi_constraints_dict[kpi][1]\n",
    "                \n",
    "                df = pd.DataFrame()\n",
    "\n",
    "                data = datas.loc[(datas[list(dim_dict)] == pd.Series(dim_dict)).all(axis=1)]\n",
    "                df['y'] = data.dropna(axis=0, subset =[kpi])[kpi]\n",
    "\n",
    "                df['ds'] = pd.to_datetime(data.dropna(axis=0, subset =[kpi])[\"ts\"], unit='s')\n",
    "\n",
    "                df = df.loc[(df['ds']>=start)&(df['ds']<=end)]\n",
    "                df = df.sort_values('ds')\n",
    "                df = df.reset_index()\n",
    "                scaler =  StandardScaler(with_mean = False)\n",
    "                scaler.fit(df['y'].values.reshape(-1,1)) \n",
    "                standard_deviations[dims_str] = scaler.scale_\n",
    "\n",
    "\n",
    "\n",
    "                param_grid = {  \n",
    "                'changepoint_prior_scale': [0.01, 0.1, 1.0],\n",
    "                'seasonality_prior_scale': [0.1, 1.0, 10.0, 50],\n",
    "                'seasonality_mode' : ['additive', 'multiplicative']\n",
    "                }\n",
    "                all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "                scores_dict = {i:[] for i in scores} #{'mape':[0.012, 0.07,...], 'smpae': []}\n",
    "                for params in all_params:\n",
    "                    m = run_prophet_funct(df, params, daily_fourier_order, weekly_fourier_order, True,  country_name)\n",
    "                    if float(row['missing_data_ratio_all'])<0.15:\n",
    "                        df_cv = cross_validation(m,initial='16 days', horizon='2 days', period = '2 days', parallel=\"processes\")\n",
    "                    else:\n",
    "                        df_cv = cross_validation(m,initial='16 days', horizon='1 days', period = '1 days', parallel=\"processes\")\n",
    "                    df_p = performance_metrics(df_cv, rolling_window=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    for i in scores:\n",
    "                        if i!='mape':\n",
    "                            scores_dict[i].append(df_p[i].values[0])\n",
    "                        elif (i == 'mape') and ('mape' in df_p.columns):\n",
    "                            scores_dict[i].append(df_p[i].values[0])\n",
    "                        else:\n",
    "                            scores = [i for i in scores if i!='mape']\n",
    "\n",
    "\n",
    "\n",
    "                    m = run_prophet_funct(df, params, daily_fourier_order, weekly_fourier_order, False, country_name)\n",
    "                    if float(row['missing_data_ratio_all'])<0.15:\n",
    "                        df_cv = cross_validation(m,initial='16 days', horizon='2 days', period = '2 days', parallel=\"processes\")\n",
    "                    else:\n",
    "                        df_cv = cross_validation(m,initial='16 days', horizon='1 days', period = '1 days', parallel=\"processes\")\n",
    "                    df_p = performance_metrics(df_cv, rolling_window=1)\n",
    "                    for i in scores:\n",
    "                        if i!='mape':\n",
    "                            scores_dict[i].append(df_p[i].values[0])\n",
    "                        elif (i == 'mape') and ('mape' in df_p.columns):\n",
    "                            scores_dict[i].append(df_p[i].values[0])\n",
    "                        else:\n",
    "                            scores = [i for i in scores if i!='mape']\n",
    "\n",
    "                #megnezzuk, hogy melyik parameter a legjobb, hozzaveve a weekend-weekday elvalasztast is\n",
    "                all_params = [dict(zip(list(param_grid.keys())+['weekend'], v)) for v in itertools.product(*list(param_grid.values())+[['True', 'False']])]\n",
    "                tuning_results = pd.DataFrame(all_params)\n",
    "                #vegigmegyunk a scoreokon:  \n",
    "                for score in scores:\n",
    "                    tuning_results[score] = scores_dict[score]\n",
    "                    tuning_results[score+'_rank'] = tuning_results[score].rank()\n",
    "                    tuning_results['rank'] = tuning_results[score+'_rank']\n",
    "                    #azt valasztjuk, amit a legelorebb sorolt, ha tobb van veletlenul valasztunk\n",
    "                    params = tuning_results[list(param_grid.keys())+['weekend']].loc[tuning_results['rank'].idxmin()]\n",
    "                    #mivel ezt kulon kell futattni, elmentjuk, hogy van-e weekend\n",
    "                    weeknd = params['weekend']\n",
    "                    #ujra fut a prophet\n",
    "                    if weeknd:\n",
    "                        m = run_prophet_funct(df, params[:-1], daily_fourier_order, weekly_fourier_order, True, country_name)\n",
    "                        future = make_future(m, end, 168)\n",
    "                        future['weekday'] = future['ds'].apply(is_weekday)\n",
    "                        future['weekend'] = ~future['ds'].apply(is_weekday)\n",
    "                        forecast = m.predict(future)\n",
    "                        \n",
    "\n",
    "                    else:\n",
    "                        m = run_prophet_funct(df, params[:-1], daily_fourier_order, weekly_fourier_order, False, country_name)\n",
    "                        future = make_future(m, end, 168)\n",
    "                        forecast = m.predict(future)\n",
    "                    \n",
    "                    \n",
    "                    if params['seasonality_mode'] == 'multiplicative':\n",
    "                        multiplicative_condition = mult_cond(forecast, df, percent, lower, upper)\n",
    "                        #ellenorizzuk a feltetelek teljesuleset\n",
    "                        if multiplicative_condition:\n",
    "                            #? Miért a seasonality_mode == \"additive\" elemeit nézzük itt?\n",
    "\n",
    "                            #! Mintha meg lenne ismételve a forecastolás itt és egy tabbal kijjebb\n",
    "                            tuning_results_additive = tuning_results.loc[tuning_results['seasonality_mode']=='additive']\n",
    "                            #majd kivalasztjuk a legjobbat\n",
    "                            params = tuning_results_additive[list(param_grid.keys())+['weekend']].loc[tuning_results_additive['rank'].idxmin()]\n",
    "                            #es tanitunk, vot ma\n",
    "                            weeknd = params['weekend']\n",
    "                            if weeknd:\n",
    "                                m = run_prophet_funct(df, params[:-1], daily_fourier_order, weekly_fourier_order,is_weekend =  True, country_name = country_name)\n",
    "                                future = make_future(m, end, 168)\n",
    "                                future['weekday'] = future['ds'].apply(is_weekday)\n",
    "                                future['weekend'] = ~future['ds'].apply(is_weekday)\n",
    "                                forecast = m.predict(future)\n",
    "\n",
    "                            else:\n",
    "                                m = run_prophet_funct(df, params[:-1], daily_fourier_order, weekly_fourier_order, False, country_name)\n",
    "                                future = make_future(m, end, 168)\n",
    "                                forecast = m.predict(future)\n",
    "                            \n",
    "\n",
    "                    if params['seasonality_mode'] == 'additive':\n",
    "                        additive_condition = add_cond(forecast, percent, lower, upper)\n",
    "                        #check conditions and fix\n",
    "                        if additive_condition:\n",
    "                            if len(m.changepoints[np.abs(np.nanmean(m.params['delta'], axis=0)) >= 0.01].values)==0:\n",
    "                                last_changepoint = start\n",
    "                            else:\n",
    "                                last_changepoint = m.changepoints[np.abs(np.nanmean(m.params['delta'], axis=0)) >= 0.01].values[-1]\n",
    "                            minimum = forecast.set_index('ds')[end:]['additive_terms'].quantile(0.05)\n",
    "                            maximum = forecast.set_index('ds')[end:]['additive_terms'].quantile(0.95)\n",
    "                            last_point =((forecast.set_index('ds')[last_changepoint:]['trend']+minimum>lower)\n",
    "                                 & (forecast.set_index('ds')[last_changepoint:]['trend']+maximum<upper))[::-1].idxmax()\n",
    "\n",
    "                            forecast.loc[forecast['ds']>last_point, 'trend'] = forecast.loc[forecast['ds']==last_point, 'trend'].values[0]\n",
    "                            forecast['yhat'] = (forecast['trend']*(1+forecast['multiplicative_terms'])+forecast['additive_terms'])\n",
    "                    forecast['yhat'] = forecast['yhat'].clip(lower = lower, upper = upper)\n",
    "                    all_predictions = all_predictions.join(forecast[['ds', 'yhat']].rename(columns={'yhat':dims_values_str}).set_index('ds'))\n",
    "                    all_errors = all_errors.join(pd.Series(scaler.transform((df.set_index('ds')['y']-forecast.set_index('ds')['yhat']).values.reshape(-1,1)).T[0], index=forecast['ds'], name = dims_values_str))\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
