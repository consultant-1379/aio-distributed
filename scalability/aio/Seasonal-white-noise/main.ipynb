{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = \"../../../../data/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prophet\n",
    "import sys\n",
    "\n",
    "sys.modules['fbprophet'] = prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from configs.kpi_constraints_dict import kpi_constraints_dict\n",
    "from configs.bad_direction_kpi_dict import bad_direction_kpi_dict\n",
    "from configs.functions import make_future, run_prophet_funct\n",
    "from configs.functions import add_cond, mult_cond, is_weekday\n",
    "from configs.functions import preprocess_data, hparam_tuning\n",
    "\n",
    "\n",
    "import logging, sys, os\n",
    "import time\n",
    "\n",
    "from uuid import uuid4\n",
    "\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "from dask.distributed import TimeoutError, CancelledError\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "flatten = lambda t: [item for sublist in t for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask client init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client( dashboard_address = ':44594', n_workers = 68, threads_per_worker = 2 ) #scheduler_address=':37243'\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"4weeks-subs_mcc-anon.csv\"#\"4weeks-subs_mcc-anon.csv\"\n",
    "datas = pd.read_csv(data_loc + file) #pd.read_csv( \"../Data/\" + file )\n",
    "print(\"-\"*30,\"DF READ\",\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_store = pd.read_csv(data_loc + \"metadata-anon.csv\")#pd.read_csv('../Data/metadata_anon.csv')\n",
    "# Mixed datatypes in the dimension_name col: floats and OrderedDict as str (need eval)\n",
    "\n",
    "# Get rid of all irrelevant metadata\n",
    "metadata_store = metadata_store[ metadata_store.model_type == 'seasonal_prophet' ]\n",
    "metadata_store = metadata_store[ metadata_store.path == file ]\n",
    "\n",
    "# evaluate str to OrderedDict\n",
    "metadata_store.dimension_name = metadata_store.dimension_name.map(str).map(lambda element: eval(element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_percentage_param = 0.3\n",
    "DAILY_FOURIER_ORDER = 3\n",
    "WEEKLY_FOURIER_ORDER = 5\n",
    "COUNTRY_NAME = 'USA'\n",
    "percent = 0.1\n",
    "scores = ['mae'] #['mdape', 'mape', 'smape', 'mae']\n",
    "predictions_write_to = '' \n",
    "errors_write_to = ''\n",
    "write_to = 'prophet_results2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = pd.to_datetime(metadata_store['ts'].values[0], unit='s')\n",
    "ts = metadata_store['ts'].values[0]\n",
    "start = end - pd.Timedelta(4, unit = 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid =    {  \n",
    "                'changepoint_prior_scale': [0.01, 0.1, 1.0],\n",
    "                'seasonality_prior_scale': [0.1, 1.0, 10.0, 50],\n",
    "                'seasonality_mode' : ['additive', 'multiplicative'],\n",
    "                }\n",
    "\n",
    "all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "all_params2 = [dict(zip(list(param_grid.keys())+['weekend'], v)) for v in itertools.product(*list(param_grid.values())+[[True, False]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_uuid_df = pd.DataFrame( { str(uuid4()): p  for p in all_params2}).T   # p_uuid: 1 id per parameter combination  for tracking - same for all models with the same hparams, regardless of timeseries\n",
    "\n",
    "p_uuid_df.iloc[0].to_dict()\n",
    "for idx, row in p_uuid_df.head().iterrows():\n",
    "    print(idx, row.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_uuid_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_training(df, row, p_uuid_df):\n",
    "    \"\"\"\n",
    "    Submit every model for training in dask.\n",
    "\n",
    "    df - timeseries dataframe in prophet ready format\n",
    "    row - metadata row\n",
    "    p_uuid_df - dataframe containing all parameter-dict and id\n",
    "    \"\"\"\n",
    "    df_p_dict = {} # store and return performance metrics of models\n",
    "    \n",
    "    # submit tasks to the client\n",
    "    for p_uuid, params_row in p_uuid_df.iterrows(): # we use all_params2 here, which have the weekend param too\n",
    "        \n",
    "        params = params_row.to_dict()\n",
    "        df_p = hparam_tuning(df, params, row,\n",
    "                                is_weekend = True,\n",
    "                                parallel = \"dask\",\n",
    "                                daily_fourier_order=DAILY_FOURIER_ORDER,\n",
    "                                weekly_fourier_order=WEEKLY_FOURIER_ORDER)\n",
    "        df_p_dict[ p_uuid ] = df_p\n",
    "\n",
    "        \n",
    "    return df_p_dict\n",
    "    \n",
    "\n",
    "def resubmit_training(df, row, params_row):\n",
    "\n",
    "    \"\"\"\n",
    "    Submit one model for training in dask\n",
    "    \"\"\"\n",
    "\n",
    "    params = params_row.to_dict()\n",
    "\n",
    "    df_p = hparam_tuning(df, params, row,\n",
    "                            is_weekend = True,\n",
    "                            parallel = \"dask\",\n",
    "                            daily_fourier_order=DAILY_FOURIER_ORDER,\n",
    "                            weekly_fourier_order=WEEKLY_FOURIER_ORDER)\n",
    "\n",
    "    return df_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myForecast(df, tuning_results,\n",
    "               row, score):\n",
    "\n",
    "    kpi = row[\"kpi_name\"]\n",
    "    dim_dict = row[\"dimension_name\"]\n",
    "\n",
    "    multiplicative_condition = False\n",
    "    additive_condition = False\n",
    "    lower = kpi_constraints_dict[kpi][0]\n",
    "    upper = kpi_constraints_dict[kpi][1]\n",
    "\n",
    "    tuning_results[score+'_rank'] = tuning_results[score].rank()\n",
    "    tuning_results['rank'] = tuning_results[score+'_rank']\n",
    "\n",
    "    params = tuning_results.loc[ tuning_results[\"rank\"].idxmin(), \n",
    "                                    [ \"changepoint_prior_scale\",\n",
    "                                    \"seasonality_prior_scale\",\n",
    "                                    \"seasonality_mode\",\n",
    "                                    \"weekend\"] ].to_dict()\n",
    "    weeknd = params['weekend']\n",
    "    params.pop(\"weekend\", None)\n",
    "\n",
    "    if weeknd:\n",
    "        m = run_prophet_funct(df, params, \n",
    "                                DAILY_FOURIER_ORDER, \n",
    "                                WEEKLY_FOURIER_ORDER,\n",
    "                                True,\n",
    "                                COUNTRY_NAME)\n",
    "        future = make_future(m, end, 168)\n",
    "        future['weekday'] = future['ds'].apply(is_weekday)\n",
    "        future['weekend'] = ~future['ds'].apply(is_weekday)\n",
    "        forecast = m.predict(future)\n",
    "    \n",
    "    else:\n",
    "        m = run_prophet_funct(df, params, \n",
    "                                DAILY_FOURIER_ORDER, \n",
    "                                WEEKLY_FOURIER_ORDER, \n",
    "                                False, \n",
    "                                COUNTRY_NAME)\n",
    "        future = make_future(m, end, 168)\n",
    "        forecast = m.predict(future)\n",
    "    \n",
    "    if params['seasonality_mode'] == 'multiplicative':\n",
    "        multiplicative_condition = mult_cond(forecast, df, percent, lower, upper, end)# ha mult_cond - > True, akkor NEM multiplikat√≠v\n",
    "        #ellenorizzuk a feltetelek teljesuleset\n",
    "        if multiplicative_condition:\n",
    "            \n",
    "            tuning_results_additive = tuning_results.loc[tuning_results['seasonality_mode']=='additive']\n",
    "            #majd kivalasztjuk a legjobbat\n",
    "            params = tuning_results_additive[list(param_grid.keys())+['weekend']].loc[tuning_results_additive['rank'].idxmin()].to_dict()\n",
    "            #es tanitunk, vot ma\n",
    "            weeknd = params.pop('weekend')\n",
    "            if weeknd:\n",
    "                m = run_prophet_funct(df, params, \n",
    "                                        daily_fourier_order = DAILY_FOURIER_ORDER, \n",
    "                                        weekly_fourier_order = WEEKLY_FOURIER_ORDER,\n",
    "                                        is_weekend = True, \n",
    "                                        country_name = COUNTRY_NAME)\n",
    "                future = make_future(m, end, 168)\n",
    "                future['weekday'] = future['ds'].apply(is_weekday)\n",
    "                future['weekend'] = ~future['ds'].apply(is_weekday)\n",
    "                forecast = m.predict(future)\n",
    "\n",
    "            else:\n",
    "                m = run_prophet_funct(df, params, \n",
    "                                        daily_fourier_order = DAILY_FOURIER_ORDER, \n",
    "                                        weekly_fourier_order = WEEKLY_FOURIER_ORDER,\n",
    "                                        is_weekend = False, \n",
    "                                        country_name = COUNTRY_NAME)\n",
    "                future = make_future(m, end, 168)\n",
    "                forecast = m.predict(future)\n",
    "    \n",
    "    if params['seasonality_mode'] == 'additive':\n",
    "        additive_condition = add_cond(forecast, percent, lower, upper, end)\n",
    "        #check conditions and fix\n",
    "        if additive_condition:\n",
    "            if len(m.changepoints[np.abs(np.nanmean(m.params['delta'], axis=0)) >= 0.01].values)==0:\n",
    "                last_changepoint = start\n",
    "            else:\n",
    "                last_changepoint = m.changepoints[np.abs(np.nanmean(m.params['delta'], axis=0)) >= 0.01].values[-1]\n",
    "            minimum = forecast.set_index('ds')[end:]['additive_terms'].quantile(0.05)\n",
    "            maximum = forecast.set_index('ds')[end:]['additive_terms'].quantile(0.95)\n",
    "            last_point =((forecast.set_index('ds')[last_changepoint:]['trend']+minimum>lower)\n",
    "                    & (forecast.set_index('ds')[last_changepoint:]['trend']+maximum<upper))[::-1].idxmax()\n",
    "\n",
    "            forecast.loc[forecast['ds']>last_point, 'trend'] = forecast.loc[forecast['ds']==last_point, 'trend'].values[0]\n",
    "            forecast['yhat'] = (forecast['trend']*(1+forecast['multiplicative_terms'])+forecast['additive_terms'])\n",
    "    \n",
    "    forecast['yhat'] = forecast['yhat'].clip(lower = lower, upper = upper)\n",
    "    \n",
    "    \n",
    "    scaler =  StandardScaler(with_mean = False) # RobustScaler\n",
    "    scaler.fit(df['y'].values.reshape(-1,1))\n",
    "\n",
    "    df = df.set_index(\"ds\")\n",
    "    forecast = forecast.set_index(\"ds\")\n",
    "\n",
    "    \n",
    "    results = pd.DataFrame( index = forecast.index,\n",
    "                            columns = [\"kpi_name\", \"dimension_name\", \"ground_truth\",\n",
    "                                        \"pred\", \"error\", \"gt_wo_trend\", \"pred_wo_trend\"])\n",
    "    \n",
    "    forecast[\"kpi_name\"] = [kpi] * len(forecast)\n",
    "    forecast[\"dimension_name\"] = [dim_dict] * len(forecast)\n",
    "    forecast[\"ground_truth\"] = df.y\n",
    "\n",
    "    forecast.rename(columns = {\"yhat\":\"pred\"}, inplace = True)\n",
    "    forecast[\"error\"] = scaler.transform(forecast.pred.values.reshape(-1,1))\n",
    "    forecast[\"ground_truth_noise\"] = df.y - forecast.trend - forecast.holidays - forecast.daily\n",
    "    forecast[\"pred_noise\"] = forecast.pred - forecast.trend - forecast.holidays - forecast.daily\n",
    "\n",
    "    forecast.reset_index(inplace = True)\n",
    "    #forecast[\"pred_wo_trend\"] = 6\n",
    "\n",
    "    #kpi n√©v, dim , ts, trend, szezon, zaj, holiday, eredeti y\n",
    "    return forecast\n",
    "\n",
    "    #pd.Series(scaler.transform((df.set_index('ds')['y']-forecast.set_index('ds')['yhat']).values.reshape(-1,1)).T[0], index=forecast['ds'], name = dims_values_str))\n",
    "    #scaler.transform((df['y']-forecast['yhat']).values.reshape(-1,1)).T[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking training status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_store[\"failed\"] = [False] * len(metadata_store)\n",
    "metadata_store[\"num_fails\"] = [0] * len(metadata_store)\n",
    "metadata_store[\"last_failed_ts\"] = [None] * len(metadata_store)\n",
    "metadata_store[\"ts_uuid\"] = metadata_store[\"failed\"].map(lambda x : str(uuid4()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing dataframes, storing them by timeseries id\n",
    "\n",
    "df_dict = {}\n",
    "for  _, mdrow in metadata_store.iterrows():\n",
    "    df = preprocess_data( datas, mdrow, start, end )\n",
    "    df_dict[ mdrow.ts_uuid ] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskgroups = {}\n",
    "\n",
    "for  _, mdrow in metadata_store.iterrows():\n",
    "    df = df_dict[ mdrow.ts_uuid ]\n",
    "\n",
    "    taskgroup = dask.delayed(submit_training)( df, mdrow, p_uuid_df )\n",
    "    taskgroups[ mdrow.ts_uuid ] = ( taskgroup )\n",
    "\n",
    "compute = dask.compute( taskgroups )[0]\n",
    "time.sleep(len(metadata_store)*0.5) # compute finishes before all submitted task, wait a little\n",
    "#! this sleep time should depend on compute performance, in the case of a 72 core Intel Xeon E5-2697 a timeseries took 6 s on average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recomputing unfinished tasks\n",
    "\n",
    "Sometimes, a few of the tasks is timed out. We need to check all whether they are finished, if not recompute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_status = lambda : pd.Series([ [ fut.status for p_uuid, fut in compute[ts_uuid].items()] for ts_uuid in compute.keys() ]).explode().value_counts()\n",
    "failed_tasks = lambda : [[ (ts_uuid,p_uuid) for p_uuid, fut in compute[ts_uuid].items() if fut.status != \"finished\"] for ts_uuid in compute.keys() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tg_finished(ts_uuid, compute):\n",
    "    \"\"\"Checking if all tasks are finished\"\"\"\n",
    "    return all([ fut.status == \"finished\" for p_uuid, fut in compute[ts_uuid].items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zookeeper_table = pd.DataFrame(columns = [\"ts_uuid\", \"p_uuid\", \"ts\", \"kpi\", \"dim\", \"params\", \"failed\", \"num_failed\", \"last_failed_ts\"]) \n",
    "# here ts in ts_uuid stands for timeseries, while ts is timestamp. ü¶Ü\n",
    "# failed contains the status of the last retry\n",
    "zookeeper_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_zk_table( zookeeper_table, compute ):\n",
    "    status_df = pd.DataFrame(compute).applymap(lambda x: x.status)\n",
    "    unfinished_tasks = [(status_df.index[x], status_df.columns[y]) for x, y in zip(*np.where(status_df.values != 'finished'))] \n",
    "\n",
    "    for _, row in zookeeper_table.iterrows():\n",
    "        ts_uuid = row.ts_uuid\n",
    "        p_uuid  = row.p_uuid\n",
    "        \n",
    "        if (p_uuid, ts_uuid) not in unfinished_tasks:\n",
    "            mask = ((ts_uuid == zookeeper_table.ts_uuid) & (p_uuid == zookeeper_table.p_uuid))\n",
    "\n",
    "            zk_row = row.copy()\n",
    "            zk_row[\"failed\"] = False\n",
    "            zookeeper_table[ mask ] = zk_row\n",
    "\n",
    "    for p_uuid, ts_uuid in unfinished_tasks:\n",
    "\n",
    "        if (ts_uuid in zookeeper_table.ts_uuid.values) and (p_uuid in zookeeper_table.p_uuid.values):\n",
    "\n",
    "            mask = ((ts_uuid == zookeeper_table.ts_uuid) & (p_uuid == zookeeper_table.p_uuid))\n",
    "            zk_row = zookeeper_table[ mask ].copy()\n",
    "            zk_row.num_failed += 1\n",
    "            zk_row.last_failed_ts = int(time.time())\n",
    "\n",
    "            zk_row.bad_status = compute[ts_uuid][p_uuid].status\n",
    "            zookeeper_table[ mask ] = zk_row\n",
    "\n",
    "        else:\n",
    "            mdrow = metadata_store[ metadata_store.ts_uuid == ts_uuid ].copy()\n",
    "\n",
    "            task_to_retry = { \"ts_uuid\": ts_uuid, \"p_uuid\": p_uuid, \"ts\": mdrow.ts.values[0], \n",
    "                                \"kpi\": mdrow.kpi_name.values[0], \"dim\": mdrow.dimension_name.values[0],\n",
    "                                \"params\": p_uuid_df.loc[p_uuid].to_dict(), \"failed\": True,\n",
    "                                \"num_failed\": 1, \"last_failed_ts\": int(time.time()),\n",
    "                                \"bad_status\": compute[ts_uuid][p_uuid].status }\n",
    "            zookeeper_table = zookeeper_table.append(pd.Series(task_to_retry), ignore_index=True)\n",
    "            \n",
    "    return zookeeper_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_unfinished_tasks( compute ): \n",
    "            \n",
    "    status_df = pd.DataFrame(compute).applymap(lambda x: x.status)\n",
    "    unfinished_tasks = [(status_df.index[x], status_df.columns[y]) for x, y in zip(*np.where(status_df.values != 'finished'))]  # tuple of (p_uuid, ts_uuid)\n",
    "\n",
    "    re_taskgroup = {}\n",
    "\n",
    "    for p_uuid, ts_uuid in unfinished_tasks:\n",
    "        df = df_dict[ ts_uuid ]\n",
    "        row = metadata_store[ metadata_store.ts_uuid == ts_uuid ]\n",
    "        params_row = p_uuid_df.loc[ p_uuid ]\n",
    "\n",
    "        if ts_uuid not in re_taskgroup:\n",
    "            re_taskgroup[ts_uuid] = {}\n",
    "        re_taskgroup[ts_uuid][p_uuid] = dask.delayed(resubmit_training)( df, row, params_row )\n",
    "\n",
    "    re_compute = dask.compute(re_taskgroup)[0] # recompute previously unfinished tasks\n",
    "\n",
    "    # Putting back new tasks - they must be mostly finished, there might be still unfinished though, then we need to \n",
    "    for ts_uuid, tg in re_compute.items():\n",
    "        \n",
    "        for p_uuid, task in tg.items():\n",
    "            compute[ts_uuid][p_uuid] = re_compute[ts_uuid][p_uuid]\n",
    "        \n",
    "    return compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_status = [\"pending\", \"cancelled\", \"lost\", \"error\", \"newly-created\", \"closing\", \"connecting\", \"running\", \"closed\" ]\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    print(i)\n",
    "    print(task_status())\n",
    "\n",
    "    zookeeper_table = refresh_zk_table(zookeeper_table, compute)\n",
    "\n",
    "\n",
    "    #check if there is an unfinished task\n",
    "    if not any(status in task_status() for status in bad_status):\n",
    "        break\n",
    "\n",
    "    status_df = pd.DataFrame(compute).applymap(lambda x: x.status)\n",
    "    unfinished_tasks = [(status_df.index[x], status_df.columns[y]) for x, y in zip(*np.where(status_df.values != 'finished'))]\n",
    "\n",
    "    compute = retry_unfinished_tasks(compute)\n",
    "\n",
    "    \n",
    "    \n",
    "    time.sleep(0.2 * len(unfinished_tasks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering results from distributed memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_df = pd.DataFrame(compute).applymap(lambda x: x.status)\n",
    "finished_tasks = [(status_df.index[x], status_df.columns[y]) for x, y in zip(*np.where(status_df.values == 'finished'))]  # tuple of (p_uuid, ts_uuid)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for p_uuid, ts_uuid in finished_tasks: \n",
    "    \n",
    "    if ts_uuid not in results:\n",
    "        results[ts_uuid] = {}\n",
    "\n",
    "    results[ts_uuid][p_uuid] = compute[ts_uuid][p_uuid].result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts, taskgroup in results.items():\n",
    "    for p_uuid, task in taskgroup.items():\n",
    "        params = p_uuid_df.loc[ p_uuid ]\n",
    "\n",
    "        \n",
    "        results[ts][p_uuid] = pd.concat([task.T, params]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated_results  = { ts_uuid : pd.concat([ v for k,v in results[ ts_uuid ].items() ]).reset_index(drop=True) for ts_uuid in results }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_forecast = {}\n",
    "for _,mdrow in metadata_store.iterrows(): \n",
    "    df = df_dict[ mdrow.ts_uuid ]\n",
    "    tuning_results = concated_results[ mdrow.ts_uuid ]\n",
    "\n",
    "    past_ts = dask.delayed(myForecast)(df, tuning_results, mdrow, \"mae\")\n",
    "    timeseries_forecast[mdrow.ts_uuid] = past_ts\n",
    "\n",
    "timeseries_forecast = dask.compute(timeseries_forecast)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dprophet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e72a233740408964b8020143c4e5759c07dd6f95d8defc3861e97068946a47f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
